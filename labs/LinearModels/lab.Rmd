---
title: "Linear regression Lab"
author:
 - "Andrea SÃ¡nchez-Tapia"
 - "Paulinha Lemos-Costa"
 - "Sara Mortara"
 - "Diogo Melo"
date: "2025-01-28"
output:
    distill::distill_article:
      toc: true
      toc_depth: 3
---


```{r setup, echo=FALSE}
library(rmarkdown)
knitr::opts_chunk$set(eval = TRUE)
# clipboard
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "Copy code <i class=\"fa fa-clipboard\"></i>",
    success_text = "Copied! <i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "Not copied ðŸ˜• <i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
  )
```

# Simple linear models

Linear regressions in R assume a linear relationship between one or more predictor variables $X_i$ and a response variable $y$. 

$$y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta x_i$$

The first element of this model describes how the response variable $Y$ can be modeled as a normal random variable with mean and standard deviation. 

The second element of this model adds the effect of the predictor variable $X$ on $Y$, based on the causal relationship 
$X \rightarrow Y$. In this simple case, the relationship is linear and can be described by the equation of a line with intercept $\alpha$ and slope $\beta$.

## Generating the dataset

In the introductory lecture for linear models, we used an example dataset to fit a simple regression model.

_The dataset itself was generated according to the specifications of the model above_. We are going to recreate the dataset and some of the calculations presented during the class. 

```{r create_dataset}
set.seed(4) #ensures the result is reproducible. The random number generation can vary between Operative systems so maybe there will be different datasets in the classroom
N = 30
x <- runif(N, 0, 5)
n <- length(x)
mu <- 1.2 + 3.5 * x
y <- rnorm(N, mean = mu, sd = 3)
lm_data <- data.frame(x,  mu, y, res = y - mu)
```

Create a plot for the dataset

```{r plot, fig.asp=1}
plot(y ~ x, pch = 19)
```


As you can see, plotting the data suggests a linear relationship between these two variables

## Regression coefficients

We can calculate the correlation coefficients using several methods. The Ordinary Least Squares (OLS) method is the most common, and provides analitical solutions for the coefficients. The slope is the ratio of the covariance between $x$ and $y$ and the variance of $x$. The intercept is the mean of $y$ minus the product of the slope and the mean of $x$.

$$\widehat{\beta}= \frac{Cov(x, y)}{Var(x)} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$

and 

$$
\hat{\alpha} = \bar{y} - \hat{\beta}\bar{x} 
$$


```{r beta0 and beta1}
beta_hat <- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x)) ^ 2)
beta_hat

# This is just the ratio of the covariance between x and y and the variance of x
cov(x, y) / var(x)

# Since mu = alpha + beta*x: 
alpha_hat <- mean(y) - beta_hat * mean(x)
alpha_hat
```

Add a regression line with the values of the coefficients you calculated

```{r OLS, fig.asp=1}
plot(y ~ x, pch = 19)
abline(a = alpha_hat, b = beta_hat, col = 2, lwd = 2)
```

## The sample is not the population 

When we generated the dataset ([here](#generating-the-dataset)), $\mu$ was specified as: 

```r
mu <- 1.2 + 3.5 * x_i
```

This means $\alpha = 1.2$ and $\beta = 3.5$. How do these values compare to the coefficients you just calculated? 

Add a population line to the plot you previously created by completing the code below: 

```r
plot(y ~ x, pch = 19)
abline(a = alpha_hat, b = beta_hat, col = 2)
abline(a = ..., b = ..., col = "blue")
```

See the code by clicking __Show code__: 

```{r regression_line, code_folding = TRUE, fig.asp = 1}
plot(y ~ x, pch = 19)
abline(a = alpha_hat, b = beta_hat, col = 2, lwd = 2)
abline(a = 1.2, b = 3.5, col = "blue", lwd = 2)
```

## Fitting a OLS linear model in R

Function `lm()` fits regression models using Ordinary Least Squares. 
The values of the coefficients can be found using function `coef()`.

```{r model_coefs}
mod <- lm(y ~ x)
coef(mod)
```

Compare this result with the calculations you did previously. 

## Model diagnostics

Before making inferences with your fitted model, it is also a good
practice to make some model diagnostics. For linear regression, the
most basic diagnostic is to check if the residuals follows a normal
distribution with zero mean and a constant standard deviation. 

Because the normal distribution has a parameter that corresponds to
the expected value, and that is independent of the other parameter,
the linear regression model we have been writing as:

$$
\begin{align}
y_i & \sim Normal(\mu_i, \sigma) \\
\mu_i & = \sum_{j=0}^{J} \beta_{j} x_{ij} \\
\end{align}
$$

Can be also written as:

$$
\begin{align}
y_i & = \sum \beta_jx_{ij} + \epsilon \\
\epsilon & \sim Normal(0, \sigma) \\
\end{align}
$$

That is, the values of the response $y_i$ are each a weighted sum of the predictor variables
$x_j$ plus a random Gaussian residual $\epsilon$. To express the
random variation symmetrically around the expected value, this
residual has a mean value of zero, and a fixed standard deviation.

To check if this assumption is true, we plot the residuals of the
fitted model as a function of the predicted values. This plot should
show a point cloud of constant width around zero in the Y-axis. You
can check this assumption applying the function `plot` to the object
that stored the fitted model:


```{r res}
plot(mod, which = 1)
```

You can also check the normality of the residuals with a
qq-plot. Normal data should lie in a straight line in this plot:

```{r diag2}
plot(mod, which = 2)
```

What is your overall evaluation of the normality of the residuals?

You can find an excellent explanation of these and other diagnostic
plots for linear regression
[here](https://data.library.virginia.edu/diagnostic-plots/)

# Fitting models using Bayesian methods

In the previous section, we fitted a linear model using the OLS method.
Now, we are going to fit the same model using a Bayesian approach.

The Bayesian approach to linear regression is based on the posterior
distribution of the parameters $\alpha$ and $\beta$. This posterior
distribution is proportional to the product of the likelihood of the
data given the parameters and the prior distribution of the parameters.

The likelihood of the data given the parameters is the same as the one
used in the OLS method. The prior distribution of the parameters is 
usually a normal distribution with mean zero and a fixed standard deviation.

We will generate a new data set with known parameters to illustrate some differences between the OLS and Bayesian methods.

```{r create_dataset2}
set.seed(4) 

x = runif(50, 50, 100)
y = 1000 + 150 * x + rnorm(50, 0, 1000)
data = data.frame(x, y)
```

Always visualize your data before fitting a model

```{r plot2}
plot(y ~ x, data = data)
```



first, lets fit the model using the OLS method:

```{r fit_ols}
mod_ols <- lm(y ~ x, data = data)

# The summary function provides a lot of information about the model, 
# including estimates, standard errors, p-values for the coefficients, 
# and residual standard deviation (sigma)
summary(mod_ols)
```

Now, let's fit the model using the rstanarm package, which fits Bayesian models using the Stan language and provides a nice set of standard weekly informative priors.

```{r fit_bayesian, messages=FALSE, eval=FALSE}
suppressPackageStartupMessages(library(rstanarm))
mod_bayesian <- stan_glm(y ~ x, data = data)

summary(mod_bayesian)
```

Let's compare the coefficient estimates of the two methods.

```{r compare, eval=FALSE}
coef(mod_ols)
coef(mod_bayesian)

library(ggplot2)

# Extract intercept and slope for both models
ols_intercept <- coef(mod_ols)[1]
ols_slope <- coef(mod_ols)[2]
bayesian_intercept <- coef(mod_bayesian)[1]
bayesian_slope <- coef(mod_bayesian)[2]


ggplot(data, aes(x, y)) + 
  geom_point() +
  geom_abline(intercept = ols_intercept, slope = ols_slope, color = "red", linetype = "solid") +
  geom_abline(intercept = bayesian_intercept, slope = bayesian_slope, color = "blue", linetype = "dashed") + 
  theme_bw()

```

How similar are the estimates of the two methods?

### Explicit model in the rethinking package

Now, let's use the rethinking package to fit the model using some standard priors we used in class.

```{r fit_rethinking, message=FALSE, eval=FALSE}
suppressPackageStartupMessages(library(rethinking))

mod_rethinking = ulam(
  alist(
    y ~ normal(mu, sigma),
    mu <- a + b * x,
    a ~ normal(0, 1),
    b ~ normal(0, 1),
    sigma ~ exponential(1)
  ),
  data = data, chains = 4, cores = 4
)
precis(mod_rethinking)
```

How are the estimates now? Awful!! 

What is happening is that the default priors in the rethinking code are too far away from the true values of the parameters. One way to avoid this type of problem is to scale the variables before fitting the model.

```{r scale, eval=FALSE}
data$x_scaled = scale(data$x)
data$y_scaled = scale(data$y)

mod_rethinking_scaled = ulam(
  alist(
    y_scaled ~ normal(mu, sigma),
    mu <- a + b * x_scaled,
    a ~ normal(0, 1),
    b ~ normal(0, 1),
    sigma ~ exponential(1)
  ),
  data = data, chains = 4, cores = 4
)
mod_stanarm_scaled = stan_glm(y_scaled ~ x_scaled, data = data)
mod_ols_scaled = lm(y_scaled ~ x_scaled, data = data)
```

Now, let's compare the estimates of the two models.

```{r compare_scaled, eval=FALSE}
coef(mod_ols_scaled)
coef(mod_stanarm_scaled)
coef(mod_rethinking_scaled)
```

Scalling variables is a good practice when fitting models with Bayesian methods. It improves computational efficiency and make defining reasonable priors easier.

### Next lab

Move on to the [multiple regression lab](lab2.html), where we go over the two examples shown in class.
---
title: "Linear regression Lab"
author:
 - "Andrea SÃ¡nchez-Tapia"
 - "Paulinha Lemos-Costa"
 - "Sara Mortara"
date: "2024-02-01"
output:
    distill::distill_article:
      toc: true
      toc_depth: 3
---


```{r setup, echo=FALSE}
library(rmarkdown)
knitr::opts_chunk$set(eval = TRUE)
# clipboard
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "Copy code <i class=\"fa fa-clipboard\"></i>",
    success_text = "Copied! <i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "Not copied ðŸ˜• <i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
  )
```

# Simple linear models

Linear regressions in R assume a linear relationship between one or more predictor variables $X_i$ and a response variable $y$. 

$$y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta x_i$$

The first element of this model describes how the response variable $Y$ can be modeled as a normal random variable with mean and standard deviation. 

The second element of this model adds the effect of the predictor variable $X$ on $Y$, based on the causal relationship 
$X \rightarrow Y$. In this simple case, the relationship is linear and can be described by the equation of a line with intercept $\alpha$ and slope $\beta$.

## Generating the dataset

In the introductory lecture for linear models, we used an example dataset to fit a simple regression model.

_The dataset itself was generated according to the specifications of the model above_. We are going to recreate the dataset and some of the calculations presented during the class. 

```{r create_dataset}
set.seed(4) #ensures the result is reproducible. The random number generation can vary between Operative systems so maybe there will be different datasets in the classroom
N = 30
x <- runif(N, 0, 5)
n <- length(x)
mu <- 1.2 + 3.5 * x
y <- rnorm(N, mean = mu, sd = 3)
lm_data <- data.frame(x,  mu, y, res = y - mu)
```

Create a plot for the dataset

```{r plot, fig.asp=1}
plot(y ~ x, pch = 19)
```


As you can see, plotting the data suggests a linear relationship between these two variables

## Regression coefficients

We can calculate the correlation coefficients using several methods. The Ordinary Least Squares (OLS) method is the most common, and provides analitical solutions for the coefficients. The slope is the ratio of the covariance between $x$ and $y$ and the variance of $x$. The intercept is the mean of $y$ minus the product of the slope and the mean of $x$.

$$\widehat{\beta}= \frac{Cov(x, y)}{Var(y)} = \frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$

and 

$$
\hat{\alpha} = \bar{y} - \hat{\beta}\bar{x} 
$$


```{r beta0 and beta1}
beta_hat <- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x)) ^ 2)
beta_hat

# This is just the ratio of the covariance between x and y and the variance of x
cov(x, y) / var(x)

# Since mu = alpha + beta*x: 
alpha_hat <- mean(y) - beta_hat * mean(x)
alpha_hat
```

Add a regression line with the values of the coefficients you calculated

```{r OLS, fig.asp=1}
plot(y ~ x, pch = 19)
abline(a = alpha_hat, b = beta_hat, col = 2, lwd = 2)
```

## The sample is not the population 

When we generated the dataset ([here](#generating-the-dataset)), $\mu$ was specified as: 

```r
mu <- 1.2 + 3.5 * x_i
```

This means $\alpha = 1.2$ and $\beta = 3.5$. How do these values compare to the coefficients you just calculated? 

Add a population line to the plot you previously created by completing the code below: 

```r
plot(y ~ x, pch = 19)
abline(a = alpha_hat, b = beta_hat, col = 2)
abline(a = ..., b = ..., col = "blue")
```

See the code by clicking __Show code__: 

```{r regression_line, code_folding = TRUE, fig.asp = 1}
plot(y ~ x, pch = 19)
abline(a = alpha_hat, b = beta_hat, col = 2, lwd = 2)
abline(a = 1.2, b = 3.5, col = "blue", lwd = 2)
```

## Fitting a linear model in R

Function `lm()` fits regression models using Ordinary Least Squares. 
The values of the coefficients can be found using function `coef()`.

```{r model_coefs}
mod <- lm(y ~ x)
coef(mod)
```

Compare this result with the calculations you did previously. 

## Model diagnostics

Before making inferences with your fitted model, it is also a good
practice to make some model diagnostics. For linear regression, the
most basic diagnostic is to check if the residuals follows a normal
distribution with zero mean and a constant standard deviation. 

Because the normal distribution has a parameter that corresponds to
the expected value, and that is independent of the other parameter,
the linear regression model we have been writing as:

$$
\begin{align}
y_i & \sim Normal(\mu_i, \sigma) \\
\mu_i & = \sum_{j=0}^{J} \beta_{j} x_{ij} \\
\end{align}
$$

Can be also written as:

$$
\begin{align}
y_i & = \sum \beta_j x_ij + \epsilon \\
\epsilon & \sim Normal(\mu, \sigma) \\
\end{align}
$$

That is, the values of the response $y_i$ are each a weighted sum of the predictor variables
$x_j$ plus a random Gaussian residual $\epsilon$. To express the
random variation symmetrically around the expected value, this
residual has a mean value of zero, and a fixed standard deviation.

To check if this assumption is true, we plot the residuals of the
fitted model as a function of the predicted values. This plot should
show a point cloud of constant width around zero in the Y-axis. You
can check this assumption applying the function `plot` to the object
that stored the fitted model:


```{r res}
plot(mod, which = 1)
```

You can also check the normality of the residuals with a
qq-plot. Normal data should lie in a straight line in this plot:

```{r diag2}
plot(mod, which = 2)
```

What is your overall evaluation of the normality of the residuals?

You can find an excellent explanation of these and other diagnostic
plots for linear regression
[here](https://data.library.virginia.edu/diagnostic-plots/)


